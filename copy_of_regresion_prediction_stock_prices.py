# -*- coding: utf-8 -*-
"""Copy of regresion-prediction_stock_prices.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/193LDXhDAtIF6yD59ekIJJdMqoBpDmJ-T

#Read in data
"""

import scipy
import numpy
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, layers, callbacks
from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
from IPython.core.pylabtools import figsize
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()
import seaborn as sns

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
  print(len(fn))

import pandas as pd

#Read in data
dataset = pd.read_csv(fn, header = 0)
print('the total numbers of rows from a CSV file: ' + str(len(dataset)))

#Drop Nan columns
dataset = dataset.dropna()
print(dataset.head)
print(dataset.dtypes)
print('Before: ' + str(len(dataset)))

# convert , to normal number

"""Clean file"""

import pandas as pd

dataset = dataset.replace(",", "", regex=True)
dataset = dataset.rename(columns = {'Data': 'Date'} )
# check format
print(dataset.dtypes)
# Change formay
dataset["Date"]= pd.to_datetime(dataset["Date"])
dataset['High'] = dataset['High'].astype('float32')
dataset['Open'] = dataset['Open'].astype('float32')
dataset['Low'] = dataset['Low'].astype('float32')
dataset['Close'] = dataset['Close'].astype('float32')
dataset['Volume'] = dataset['Volume'].astype('float32')
# check format
print(dataset.dtypes)
# sort old -> young
# dataset.sort_values(by='Date', inplace=True)
# Double check the result
dataset.head()
print(dataset)

import matplotlib.pyplot as plt

plt.figure(figsize = (18,9))
plt.plot(range(dataset.shape[0]),(dataset['Low'] + dataset['High'])/2.0)
plt.xticks(range(0,dataset.shape[0],500),dataset['Date'].loc[::500],rotation=45)
plt.xlabel('Date',fontsize=18)
plt.ylabel('Mid Price',fontsize=18)
plt.show()

train_size = int(len(dataset)*0.7)
train_dataset, test_dataset = dataset.iloc[:train_size], dataset.iloc[train_size:]

print('Dimension of train data: ',train_dataset.shape)
print('Dimension of test data: ', test_dataset.shape)

# Plot train and test data
plt.figure(figsize = (18,9))
plt.plot(train_dataset.Low, label = "Train")
plt.plot(test_dataset.Low, label = "Test")
# plt.xlabel('Time (day)')
# plt.ylabel('Daily water consumption ($m^3$/capita.day)')
plt.legend(['Train set', 'Test set'], loc='upper right')
#plt.savefig('C:/Users/nious/Documents/Medium/LSTM&GRU/2.jpg', format='jpg', dpi=1000)
plt.title('High Values in Train and Test Dataset')
plt.show()

# Split train data to X and y
X_train = train_dataset.drop(['Close','Date'], axis = 1)
print(X_train.shape)
y_train = train_dataset.loc[:,['Close']].to_numpy()
print(y_train.shape)
# Split test data to X and y
X_test = test_dataset.drop(['Close','Date'], axis = 1)
print(X_test.shape)
y_test = test_dataset.loc[:,['Close']].to_numpy()
print(y_test.shape)

train_size = int(len(X_test)*0.9)
x_dataset = X_test.iloc[train_size:]
print(x_dataset.shape)
print(train_size)
y_train_size = int(len(y_test)*0.9)
y_dataset = test_dataset.loc[:,['Close']].iloc[y_train_size:]

print(y_dataset.shape)

# Different scaler for input and output
scaler_x = MinMaxScaler(feature_range = (0,1))
scaler_y = MinMaxScaler(feature_range = (0,1))

# Fit the scaler using available training data
input_scaler = scaler_x.fit(X_train)
output_scaler = scaler_y.fit(y_train)

# Apply the scaler to training data
train_y_norm = output_scaler.transform(y_train)
train_x_norm = input_scaler.transform(X_train)

# Apply the scaler to test data
test_y_norm = output_scaler.transform(y_test)
test_x_norm = input_scaler.transform(X_test)

# Create a 3D input
def create_dataset (X, y, time_steps = 1):
    Xs, ys = [], []
    for i in range(len(X)-time_steps):
        v = X[i:i+time_steps, :]
        Xs.append(v)
        ys.append(y[i+time_steps])
    return np.array(Xs), np.array(ys)
TIME_STEPS = 30
X_test, y_test = create_dataset(test_x_norm, test_y_norm,   
                                TIME_STEPS)
X_train, y_train = create_dataset(train_x_norm, train_y_norm, 
                                  TIME_STEPS)
print('X_train.shape: ', X_test.shape)
print('y_train.shape: ', y_train.shape)
print('X_test.shape: ', X_test.shape)
print('y_test.shape: ', y_train.shape)

# Create LSTM
def create_model(units, m):
    model = Sequential()
    model.add(m (units = units, return_sequences = True,
                input_shape = [X_train.shape[1], X_train.shape[2]]))
    model.add(Dropout(0.2))
    model.add(m (units = units))
    model.add(Dropout(0.2))
    model.add(Dense(units = 1))
    #Compile model
    model.compile(loss='mse', optimizer='adamax', metrics= 'accuracy')
    return model

def create_model_bilstm(units):
    model = Sequential()
    model.add(Bidirectional(LSTM(units = units,                             
              return_sequences=True),
              input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(Bidirectional(LSTM(units = units)))
    model.add(Dense(1))
    #Compile model
    model.compile(loss='mse', optimizer='adam',  metrics= 'accuracy')
    return model

model_lstm = create_model(128, LSTM)
# model_bilstm = create_model_bilstm(128)
# model_gru = create_model(64, GRU)

# Fit LSTM
def fit_model(model):
    early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss',
                                              patience = 50)
    
    history = model.fit(X_train, y_train, 
                        epochs = 100,  
                        validation_split = 0.2, # validation_data = (test_X, test_y)
                        batch_size = 128,  
                        verbose = 2,
                        shuffle = False,
                        callbacks = [early_stop])
    return history

#the best batch_size = 128, units = 128, 128, 1, optimizer='adamax'

history_lstm = fit_model(model_lstm)
# history_bilstm = fit_model(model_bilstm)
# history_gru = fit_model(model_gru)

# Plot train loss and validation loss
def plot_loss(history):
    plt.figure(figsize = (18,9))
    plt.title('Train loss and validation loss')
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylabel('Loss')
    plt.xlabel('epoch')
    plt.legend(['Train loss', 'Validation loss'], loc='upper right')

plot_loss(history_lstm)
# plot_loss(model_bilstm)
# plot_loss (history_gru)

def plot_acc(history):  
  plt.figure(figsize = (18,9))
  plt.title('Training and Validation accuracy')
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  plt.legend(['Train acc', 'Validation acc'], loc='upper right')
  plt.xlabel('Epochs')
  plt.ylabel('Accuracy')
  plt.show()

plot_loss(history_lstm)
# plot_loss (model_bilstm)
# plot_loss (history_gru)

y_test = scaler_y.inverse_transform(y_test)
y_train = scaler_y.inverse_transform(y_train)

def prediction(model):
    prediction = model.predict(X_test)
    prediction = scaler_y.inverse_transform(prediction)
    return prediction

# prediction_lstm = prediction(model_lstm)
prediction_bilstm = prediction(model_bilstm)
# prediction_gru = prediction(model_gru)

def plot_future(prediction, model_name, y_test):
    
    plt.figure(figsize=(18, 9))
    
    range_future = len(prediction)

    plt.plot(np.arange(range_future), np.array(y_test), label='test dataset')
    plt.plot(np.arange(range_future), np.array(prediction),label='Prediction result')

    plt.title('True future vs prediction for ' + model_name)
    plt.legend(loc='upper left')
    plt.xlabel('Time (day)')
    plt.ylabel('Daily water consumption ($m^3$/capita.day)')
    #plt.savefig('C:/Users/nious/Documents/Medium/LSTM&GRU/predic_'+model_name+'.jpg', format='jpg', dpi=1000)
    
# plot_future(prediction_lstm, 'LSTM', y_test)
plot_future(prediction_bilstm, 'bilstm', y_test)
# plot_future(prediction_lstm, 'GRU', y_test)

def evaluate_prediction(predictions, actual, model_name):
    errors = predictions - actual
    mse = np.square(errors).mean()
    rmse = np.sqrt(mse)
    mae = np.abs(errors).mean()
    print(str(model_name) + ': ')
    # print('Mean Absolute Error: ' + str(errors))
    print('mse: ' + str(mse))
    print('rmse: ' + str(rmse))
    print('mae: ' + str(rmse))

# evaluate_prediction(prediction_lstm, y_test, 'LSTM')
evaluate_prediction(prediction_bilstm, y_test, 'Bidirectional LSTM')
# evaluate_prediction(prediction_gru, y_test, 'GRU')

def plot_history_future(y_train, prediction):
    plt.figure(figsize=(18, 9))    
    range_history = len(y_train)
    upperRange = range_history + len(prediction)
    range_future = list(range(range_history,upperRange))
    plt.plot(np.arange(range_history), np.array(y_train), 
             label='History')
    plt.plot(range_future, np.array(prediction),label='Prediction')
    plt.legend(loc='upper right')
    plt.xlabel('Time (day)')
    plt.ylabel('Daily water consumption ($m^3$/capita.day)')# Multi-step forecasting 
def forecast(X_input, time_steps):
    # Scale the unseen input with the scaler fitted on the train set
    X = input_scaler.transform(X_input)
    # Reshape unseen data to a 3D input
    Xs = []
    for i in range(len(X) - time_steps):
        v = X[i:i+time_steps, :]
        Xs.append(v)   
    
    X_transformed = np.array(Xs)# Make prediction for unseen data using LSTM model
    prediction = model_lstm.predict(X_transformed)
    prediction_actual = scaler_y.inverse_transform(prediction)
    return prediction_actual

# print(test_dataset.shape)
# print(test_dataset)        
prediction = forecast(x_dataset, TIME_STEPS)

plot_history_future(y_dataset, prediction)